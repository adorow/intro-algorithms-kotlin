Chapter 11. Hash Tables
=======================

Many applications require a dynamic set that supports only the dictionary operations INSERT, SEARCH, DELETE.
A hash table is an effective data structure for implementing dictionaries.
Although searching for an element in a hash table can take as long as searching for an element in a linked list - _Θ_(_n_) time in the worst case - in practice, hashing performs extremely well.
Under reasonable assumptions, the average time to search for an element in a hash table is _O_(1).

When the number of keys actually stored is small relative to the total number of possible keys, hash tables become an effective alternative to directly addressing an array, since a hash table typically uses an array of size proportional to the number of keys actually stored.
Instead of using the keys as an array index directly, the array index is _computed_ from the key.

### Direct-address tables

Direct addressing is a simple technique that works well when the universe _U_ of keys is reasonably small.
Suppose that an application needs a dynamic set in which each element has a key drawn from the universe _U_ = {0, 1, ..., _m_ - 1}, where _m_ is not too large.
We assume that no two elements have the same key.

To represent this set, we use an array, or a _**direct-access table**_, denoted by _T_[0..._m_-1], in which each position, or _**slot**_, corresponds to a key in the universe _U_.

The dictionary operations are trivial to implement:

**Pseudocode**:
>Direct-Address-Search(_T_, _k_)  
&nbsp;&nbsp;&nbsp;&nbsp;    **return** _T_[_k_]  

>Direct-Address-Insert(_T_, _x_)  
&nbsp;&nbsp;&nbsp;&nbsp;    _T_[_x_._key_] = _x_  

>Direct-Address-Delete(_T_, _x_)  
&nbsp;&nbsp;&nbsp;&nbsp;    _T_[_x_._key_] = NIL  

**In Kotlin**: [Implementation](../src/main/kotlin/chapter11/DirectAddressTable.kt) and [Tests](../src/test/kotlin/chapter10/DirectAddressTableTest.kt).

Each of these operations takes only _O_(1) time.

### Hash tables

The downside of direct-address tables is obvious: if the universe _U_ is large, storing a table of size |_U_| becomes impractical, perhaps even impossible, given the memory available on a typical computer.
Furthermore, the set of _K_ keys actually stored may be so small relative to _U_ that most of the space allocated for _T_ would be wasted.

When the set of _K_ keys stored in a dictionary is much smaller than the universe _U_ of all possible keys, a hash table requires much less storage than the direct-address table.
We can reduce the storage requirement to _θ_(|_K_|) while we maintain the benefit that searching for an element still only requires _O_(1) time.
The catch is that this is bound for the _average-case time_, whereas for direct addressing it holds for the _worst-case time_.

With direct addressing, an element with key _k_ is stored in slot _k_. 
With hashing, this element is stored in slot _h_(_k_); that is, we use a hash function h to compute the slot from the key _k_. 
Here, _h_ maps the universe _U_ of keys into the slots of a _**hash table**_ _T_[0.._m_ - 1]:

>_h_ : _U_ → {0, 1, ..., _m_ - 1}

where the size _m_ of the hash table is typically much smaller than |_U_|.
We say that an element with key _k_ _**hashes**_ to slot _h_(_k_); we also say that _h_(_k_) is the _**hash value**_ of key _k_.

There is one problem though: two keys may hash to the same slot.
This situation is called a **collision**.
Fortunately there are effective techniques for solving the conflict created by collisions.

Of course the ideal would be to try and ignore collisions altogether.
We might try to achieve this by choosing a suitable hash function _h_.
One is to make _h_ appear to be "random", minimizing the chances of collisions.
The very term "hash", evoking images of random mixing and chopping, captures the spirit of this approach.
Is important to note that a hash function must be determinist, so that for a given input _k_, the output produced is always the same.
Because |_U_| > _m_, however, there must be at least two keys with the same hash value; avoiding collisions altogether is therefore impossible.
Thus, while a well designed hash function the reduce the number of collisions, we still need a method to resolve the collisions that do occur.

### Collision resolution by chaining

In **chaining**, we place all the elements that hash to the same slot into the same linked list.
Slot _j_ contains a pointer to the head of the list of all stored elements that hash to _j_; if there are no such elements, slot _j_ contains NIL.

The dictionary operations on a hash table _T_ are easy to implement when collisions are resolved by chaining:

**Pseudocode**:
>Chained-Hash-Insert(_T_, _x_)  
&nbsp;&nbsp;&nbsp;&nbsp;    insert _x_ at the head of list _T_[_h_(_x_._key_)]  

>Chained-Hash-Search(_T_, _k_)  
&nbsp;&nbsp;&nbsp;&nbsp;    search for an element with key _k_ in list _T_[_h_(_k_)]  

>Chained-Hash-Delete(_T_, _x_)  
&nbsp;&nbsp;&nbsp;&nbsp;    delete _x_ from the list _T_[_h_(_x_._key_)]  

**In Kotlin**: [Implementation](../src/main/kotlin/chapter11/ChainedHashTable.kt) and [Tests](../src/test/kotlin/chapter10/ChainedHashTableTest.kt).

The worst-case running time for insertion is _O_(1).
The insertion procedure is fast in part because it assumes that the element _x_ is not already present in the table.
If necessary we can check this assumption (at additional cost), by searching the list looking for aan element whose key is _x_._key_.
If we assume that _x_ is the reference is a node in the list, then we can also delete an element in _O_(1) time if the lists are doubly linked.

#### Analysis of hashing with chaining

Given a table _T_ with _m_ slots that stores _n_ elements, we define the **load factor** _α_ for _T_ as _n_/_m_, that is, the average number of elements stored in a chain.
Our analysis will be in terms of _α_, which can be less than, equal to, or greater than 1.

The worst-case behaviour of hashing with chaining is terrible: all _n_ keys hash to the same slot, creating a list of length _n_.
The worst-case time for searching is thus _θ_(_n_) plus the time to compute the hash function - no better than if we used one linked list for all the elements.
Clearly, we do not use hash tables for their worst-case performance.

The average-case performance of hashing depends on how well _h_ distributes the set of keys to be stored among the _m_ slots, on the average.

For now we shall assume that any given element is equally likely to hash into any of the _m_ slots, independently of where any other element has hashed to. 
We call this the assumption of _**simple uniform hashing**_.

### Hash functions

Here starts a discussion on the design of good hash functions, and then three schemes are presented.
Two of these schemes, hashing by division and hashing by multiplication, are heuristic in nature, whereas the third scheme, universal hashing, uses randomization to provide provably good performance.

##### What makes a good hash function?

A good hash function satisfies (approximately) the assumption of simple uniform hashing: each key is equally likely to hash to any of the _m_ slots independently of where any other key has hashed to.
Unfortunately, we typically have no way to check this condition, since we rarely know the probability distribution from which the keys are drawn. 
Moreover, the keys might not be drawn independently.

Occasionally we do know the distribution.
For example, if we know that the keys are random real numbers k independently and uniformly distributed in the range 0 ≤ _k_ < 1, then the hash function

>_h_(_k_) = ⌊_km_⌋

satisfies the condition of simple uniform hashing.

In practice, we can often employ heuristic techniques to create a hash function that performs well.
Qualitative information about the distribution of keys may be useful in this design process.

##### Interpreting keys as natural numbers

Most hash functions assume that the universe of keys is the set ℕ = {0, 1, 2, ...} of natural numbers.
Thus, if the keys are not natural numbers, we find a way to interpret them as natural numbers.
For example, we can interpret a character string as an integer expressed in suitable radix notation.
Thus, we might interpret the identifier _pt_ as the pair of decimals (112, 116), since _p_ = 112 and _t_ = 116 in the ASCII character set; then expressed as a radix-128 integer, _pt_ becomes (112 · 128) + 116 = 14452.
In the context of a given application, we can usually devise some such method for interpreting each key as a (possibly large) natural number.

#### The division method

In the _**division method**_ for creating hash functions, we map a key _k_ into one of _m_ slots by taking the remainder of _k_ divided by _m_.
That is, the hash function is

>_h_(_k_) = _k_ mod _m_

For example, if the hash table has size _m_ = 12 and the key is _k_ = 100, then _h_(_k_) = 4. 
Since it requires only a single division operation, hashing by division is quite fast.

When using the division method, we usually avoid certain values of _m_.
It should, for example, not be a power of 2, since if _m_ = 2<sup>p</sup>, then _h_(_k_) is just the _p_ lowest-order bits of _k_.
Unless we know that all low-order p-bit patterns are equally likely, we are better off designing the hash function to depend on all the bits of the key.

A prime not too close to an exact power of 2 is often a good choice for _m_. 
For example, suppose we wish to allocate a hash table, with collisions resolved by chaining, to hold roughly _n_ = 2000 character strings, where a character has 8 bits.
We don’t mind examining an average of 3 elements in an unsuccessful search, and so we allocate a hash table of size _m_ = 701. 
We could choose _m_ = 701 because it is a prime near 2000/3 but not near any power of 2. 
Treating each key _k_ as an integer, our hash function would be

>_h_(_k_) = _k_ mod 701

#### The multiplication method

The **multiplication method** for creating hash functions operates in two steps.
First we multiply the key _k_ by a constant _A_ in the range 0 < _A_ < 1 and extract the fractional part of _kA_.
Then, we multiply this value by _m_ and take the floor of the result.
In short, the hash function is

>_h_(_k_) = ⌊_m_(_kA_ mod 1)⌋

where "_kA_ mod 1" means the fractional part of _kA_, that is, _kA_ - ⌊_kA_⌋.

An advantage of the multiplication method is that the value of m is not critical.
We typically choose it to be a power of 2 (_m_ = 2<sup>_p_</sup> for some integer _p_) since we can then easily implement the function on most computers as follows.
Suppose that the word size of the machine is _w_ bits and that _k_ fits into a single word.
We restrict _A_ to be a fraction of the form _s_/2<sup>_w_</sup>, where _s_ is an integer in the range 0 < _s_ < 2<sup>_w_</sup>.
We first multiply _k_ by the _w_-bit integer _s_ = _A_ · 2<sup>_w_</sup>.
The result is a _2w_-bit value _r_<sub>1</sub>2<sup>_w_</sup> + _r_<sub>0</sub>, where _r_<sub>1</sub> is the high-order word of the product and _r_<sub>0</sub> is the low-order word of the product.
The desired _p_-bit hash value consists of the _p_ most significant bits of _r_<sub>0</sub>.

Although this method works with any value of the constant _A_, it works better with some values than with others.

#### Universal hashing

If a malicious adversary chooses the keys to be hashed by some fixed hash function, then the adversary can choose _n_ keys that all hash to the same slot, yielding an average retrieval time of _Θ_(_n_).
Any fixed hash function is vulnerable to such terrible worst-case behaviour; the only effective way to improve the situation is to choose the hash function _randomly_ in a way that is _independent_ of the keys that are actually going to be stored.
This approach, called _**universal hashing**_, can yield provably good performance on average, no matter which keys the adversary chooses.

In universal hashing, at the beginning of execution we select the hash function at random from a carefully designed class of functions.
As in the case of quicksort, randomization guarantees that no single input will always evoke worst-case behaviour.
Because we randomly select the hash function, the algorithm can behave differently in each execution, even for the same input, guaranteeing good average-case performance for any input.

Let _H_ be a finite collection of hash functions that map a given universe _U_ of keys into the range {0, 1, ..., _m_ - 1}.
Such a collection is said to be _**universal**_ if for each pair of distinct keys _k_, _l_ ∈ _U_, the number of hash functions _h_ ∈ _H_ for which _h_(_k_) = _h_(_l_) is at most |_H_|/_m_.
In other words, with a hash function randomly chosen from _H_, the chance of a collision between distinct keys _k_ and _l_ is no more than the chance 1/_m_ of a collision if _h_(_k_) and _h_(_l_) were randomly and independently chosen from the set {0, 1, ..., _m_ - 1}.

#### Designing a universal class of hash functions

Do design a universal class of hash functions, we start by choosing a prime number _p_ large enough so that every possible key _k_ is in the range 0 to _p_ - 1, inclusive.
Let ℤ<sub>_p_</sub> denote the set {0, 1, ..., _p_ - 1}, and let ℤ<sup>*</sup><sub>_p_</sub> denote the set {1, 2, ..., _p_ - 1}.
Since _p_ is prime, we can solve equations modulo _p_.
Because we assume that the size of the universe of keys is greater than the number of slots in the hash table, we have _p_ > _m_.

We now define the hash function _h_<sub>_ab_</sub> for any _a_ ∈ ℤ<sup>*</sup><sub>_p_</sub> and any _b_ ∈ ℤ<sub>_p_</sub> using a linear transformation followed by reductions modulo _p_ and then modulo _m_:

>_h_<sub>_ab_</sub>(_k_) = ((_ak_ + _b_) mod _p_) mod _m_

For example, with _p_ = 17 and _m_ = 6, we have _h_<sub>3,4</sub>(8) = 5.
The family of all such hash functions is

>_H_<sub>_pm_</sub> = {_h_<sub>_ab_</sub> : _a_ ∈ ℤ<sup>*</sup><sub>_p_</sub> and _b_ ∈ ℤ<sub>_p_</sub>}

Each hash function _h_<sub>_ab_</sub> maps ℤ<sup>p</sup> to ℤ<sup>m</sup>.
This class of hash functions has the nice property that the size _m_ of the output range is arbitrary - not necessarily prime.
Since we have _p_ - 1 choices for _a_ and _p_ choices for _b_, the collection _H_<sub>_pm_</sub> contains _p_(_p_ - 1) hash functions.

### Open addressing

In _**open addressing**_, all elements occupy the hash table itself.
Meaning that each table entry contains either an element of the dynamic set or NIL.
When searching for an element, we systematically examine table slots until either we find the desired element or we have ascertained that the element is not on the table.
No lists and no elements are stored outside of the table, unlike in chaining.
Thus, in _open-addressing_, the hash table can "fill up", so that no further insertions can be made; one consequence is that the load-factor _α_ can never exceed 1.

In open addressing, instead of following pointers, we _compute_ the sequence of slots to be examined.
The extra memory freed by not storing pointers provides the hash table with a larger number of slots for the same amount of memory, potentially yielding fewer collisions and faster retrieval.

To perform insertion, we successively examine, or **probe**, the hash table until we find an empty slot in which to put the key.
Instead of being fixed in order, which requires _θ_(_n_) search time, the sequence of positions probed _depends upon the key being inserted_.
To determine which slots to probe, we extend the hash function to include the probe number (starting from 0) as a second input.
Thus the hash function becomes:

>_h_ : _U_ × {0, 1, ...., _m_ - 1} → {0, 1, ...., _m_ - 1}

With open addressing, we require thaat for every key _k_, the **probe sequence**

>〈_h_(_k_, 0), _h_(_k_, 1), ..., _h_(_k_, _m_ - 1)〉

be a permutation of 〈0, 1, ..., _m_ - 1〉, so that every hash-table position is eventually considered as a slot for a new key as the table fills up.
In the following pseudocode, we assume that every element in the hash-table _T_ is a key with no satellite information; the key _k_ is identical to the element containing key _k_.
Each slot contains either a key or NIL (if the slot is empty).

Hash-Insert takes a hash-table _T_ and a key _k_, and returns the slot number where it stored key _k_, or flags an error because the hash-table is already full.

The algorithm for searching for key _k_ probes the same sequence of slots that were used when the same key was inserted.
Therefore the search can terminate (unsuccessfully) when it finds an empty slot, since _k_ would have been inserted there and not later in its probe sequence (assuming that no keys have been deleted from the hash-table).
Hash-Search takes a hash-table _T_ and a key _k_, returning _j_ if it finds a slot containing key _k_, or NIL if _k_ was not found.

Deletion from an open-address hash-table is difficult.
When deleting a key from slot _i_, we cannot simply mark that slot empty by storing NIL.
If we did, we might be unable to retrieve any key _k_ during whose insertion we had probed slot _i_ and found it to be occupied.
We can solve this problem by marking the slot, storing in it a special value DELETED instead of NIL.
Hash-Insert would have to be modified to insert a key when a slot is marked as DELETED.
Hash-Search would still pass through it, so does not require modifications.
However, search times no longer depend on on the load factor _α_, and for this reason chaining is more commonly used as a collision resolution technique when keys must be deleted.

**Pseudocode**:
>Hash-Insert(_T_, _x_)  
&nbsp;&nbsp;&nbsp;&nbsp;    _i_= 0  
&nbsp;&nbsp;&nbsp;&nbsp;    **repeat**  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        _j_ = _h_(_k_, _i_)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        **if** _T_[_j_] == NIL  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            _T_[_j_] = _k_  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            **return** _j_  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        **else** _i_ = _i_ + 1  
&nbsp;&nbsp;&nbsp;&nbsp;    **until** _i_ == _m_  
&nbsp;&nbsp;&nbsp;&nbsp;    **error** "hash table overflow"  

>Hash-Search(_T_, _k_)  
&nbsp;&nbsp;&nbsp;&nbsp;    _i_= 0  
&nbsp;&nbsp;&nbsp;&nbsp;    **repeat**  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        _j_ = _h_(_k_, _i_)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        **if** _T_[_j_] == k  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            **return** _j_  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        _i_ = _i_ + 1  
&nbsp;&nbsp;&nbsp;&nbsp;    **until** _T_[_j_] == NIL or _i_ == _m_  
&nbsp;&nbsp;&nbsp;&nbsp;    **return** NIL  

**In Kotlin**: [Implementation](../src/main/kotlin/chapter11/OpenAddressingTable.kt) and [Tests](../src/test/kotlin/chapter11/OpenAddressingTableTest.kt).

In this analysis, we aassume _**uniform hashing**_: the probe sequence of each key is equally likely to be any of the _m_! permutations of 〈0, 1, ..., _m_ - 1〉.
Uniform hashing generalizes the notion of simple uniform hashing to a hash function that produces not just a single number but a whole probe sequence.
True uniform hashing is hard to implement, however, and in practice suitable approximations are used.

Below we start examining three commonly used techniques to compute the probe sequences required for open addressing: linear probing, quadratic probing, and double hashing.
They all guarantee that 〈_h_(_k_, 0), _h_(_k_, 1), ..., _h_(_k_, _m_ - 1)〉 is a permutation of 〈0, 1, ..., _m_ - 1〉 for each key _k_.
None of them fulfill the assumption of uniform hashing, however, since none of them is capable of generating more than _m_<sup>2</sup> different probe sequences (instead of the _m_! that uniform hashing requires).
Double hashing has the greatest number of probe sequences, and seems to give the best results.

#### Linear probing

Given an ordinary hash function _h'_ : _U_ → {0, 1, ..., _m_ - 1}, which we refer to as an _**auxiliary hash function**_, the method of _**linear probing**_ uses the hash function

>_h_(_k_, _i_) = (_h'_(_k_) + _i_) mod _m_

**In Kotlin**: [Implementation](../src/main/kotlin/chapter11/LinearProbing.kt) and [Tests](../src/test/kotlin/chapter11/LinearProbingTest.kt).

for _i_ = 0, 1, ..., _m_ - 1.
Given key _k_, we first probe _T_[_h'_(_k_)], next we probe _T_[_h'_(_k_) + 1] and so on up to slot _T_[_m_ - 1] and then we wrap around to _T_[0], _T_[1], ..., until we finally probe slot _T_[_h'_(_k_) - 1].
Because the initial probe determines the entire probe sequence, there are only _m_ distinct probe sequences.

Linear probing is easy to implement, but it suffers from a problem known as _**primary clustering**_.
Long runs of occupied slots build up, which increase the average search time.
Clusters arise because an empty slot preceded by _i_ full slots get filled next with probability (_i_ +  1)/_m_.
Long runs of occupied slots tend to get longer, and the average search times increase.

#### Quadratic probing

_**Quadratic probing**_ uses a hash function of the form

>_h_(_k_, _i_) = (_h'_(_k_) + _c_<sub>1</sub>_i_ + _c_<sub>2</sub>_i_<sup>2</sup>) mod _m_

**In Kotlin**: [Implementation](../src/main/kotlin/chapter11/QuadraticProbing.kt) and [Tests](../src/test/kotlin/chapter11/QuadraticProbingTest.kt).


where _h'_ is an auxiliary hash function, _c_<sub>1</sub> and _c_<sub>2</sub> are positive auxiliary constants, and _i_ = 0, 1, ..., _m_ - 1.
The initial position probed is _T_[_h'_(_k_)], later positions probed are offset by amounts that depend in a quadratic manner on the probe number _i_.
This method works much better than linear probing, but to make full use of the hash table, the values of _c_<sub>1</sub>, _c_<sub>2</sub> and _m_ are constrained.
Also, if two keys have the same initial probe position, then their probe sequences are the same, since _h_(_k_<sub>1</sub>, 0) = _h_(_k_<sub>2</sub>, 0) implies _h_(_k_<sub>1</sub>, _i_) = _h_(_k_<sub>2</sub>, _i_).
This property leads to a milder form of clustering, called _**secondary clustering**_.
As in linear probing, the initial probe determines the entire sequence, and so only _m_ distinct probe sequences are used.

#### Double hashing

Double hashing offers one of the best methods available for open addressing because the permutations produced have many of the characteristics of randomly chosen permutations.
_**Double hashing**_ uses a hash function of the form

>_h_(_k_, _i_) = (_h_<sub>1</sub>(_k_) + _ih_<sub>2</sub>(_k_)) mod _m_

where both _h_<sub>1</sub> and _h_<sub>2</sub> are auxiliary hash functions.
The initial probe goes to _T_[_h<sub>1</sub>_(_k_)]; successive probe positions are offset from previous positions by the amount _h<sub>2</sub>_(_k_) modulo _m_.
Thus, unlike the case of linear or quadratic probing, the probe sequence here depends in two ways upon the key _k_, since the initial probe position, the offset, or both, may vary.
The value _h_<sub>2</sub>(_k_) must be relatively prime to the hash-table size _m_ for the entire hash table to be searched.
A convenient way to ensure this condition is to let _m_ be a power of 2 and to design _h_<sub>2</sub> to always return odd numbers.
Another way is to let _m_ be prime and design _h_<sub>2</sub> so that it always returns a positive integer less than _m_.

When _m_ is prime or a power of 2, double hashing improves over linear or quadratic probing in that _Θ_(_m_<sup>2</sup>) probe sequences are used, rather than _Θ_(_m_), since each possible (_h_<sub>1</sub>(_k_), _h_<sub>2</sub>(_k_)) pair yields a distinct probe sequence.
As a result, for such values of _m_, the performance of double hashing appears to be very close to the performance of an "ideal" scheme for uniform hashing.

**In Kotlin**: [Implementation](../src/main/kotlin/chapter11/DoubleHashing.kt) and [Tests](../src/test/kotlin/chapter11/DoubleHashingTest.kt).

#### Analysis of open-address hashing

As in the analysis of chaining, we do the analysis of open addressing in terms of the load factor _α_ = _n_/_m_ of the hash table.
Of course, with open addressing, at most one element occupies each slot, and thus _n_ ≤ _m_, which implies _α_ ≤ 1.
We assume that we are using uniform hashing.
In this idealized scheme, the probe sequence 〈_h_(_k_, 0), _h_(_k_, 1), ..., _h_(_k_, _m_ - 1)〉used to insert or search for each key _k_is equally likely to be any permutation of 〈0, 1, ..., _m_ - 1〉.
A given key has a unique probing sequence, but what is meant here is that considering the probability distribution on the space of keys and the operation of the hash function on keys, each possible probe sequence is equally likely.

### Perfect hashing

Although hashing is often a good choice due to its _average-case_ performance, hashing can also provide excellent _worst-case_ performance when the set of keys is **static**: once they are stored in the table, the set of keys never changes.
Some application naturally have a set of static keys, like for example the words in aa programming language.
We call a hashing technique _**perfect hashing**_ if _O_(1) memory accesses are required to perform a search in the worst case.

To create a perfect hashing scheme, we use two levels of hashing, with universal hashing at each level.

The first level is essentially the same as for hashing with chaining: we hash the _n_ keys into _m_ slots using a hash function h carefully selected from a family of universal hash functions.

Instead of making a linked list of the keys hashing to slot _j_, however, we use a small _**secondary hash table**_ _S_<sub>_j_</sub> with an associated hash function _h_<sub>_j_</sub>.
By choosing the function _h_<sub>_j_</sub> carefully, we can guarantee that there are no collisions at the secondary level.

In order to guarantee that there are no collisions at the secondary level, however, we will need to let the size _m_<sub>_j_</sub> of hash table _S_<sub>_j_</sub> be the square of the number _n_<sub>_j_</sub> of keys hashing to slot _j_.
Although one might think that the quadratic dependence of _m_<sub>_j_</sub> on _n_<sub>_j_</sub> may seem likely to cause the overall storage requirement to be excessive, we shall show that by choosing the first-level hash function well, we can limit the expected total amount of space used to _O_(_n_).

The first-level hash function comes from the class _H_<sub>_pm_</sub>, where _p_ is a prime number greater than any key value.
Those keys hashing to slot _j_ are re-hashed into a secondary hash table _S_<sub>_j_</sub> of size _m_<sub>_j_</sub> using a hash function _h_<sub>_j_</sub> chosen from the class _H_<sub>_p_,_m_<sub>_j_</sub></sub>.

