Chapter 15. Dynamic Programming
===============================

Dynamic programming, like the divide-an-conquer method, solves problems by combining the solutions to subproblems. ("Programming" in this context refers to a tabular method, not to writing computer code.)
As seen in [Chapter 2](1.02.md) and [Chapter 4](1.04.md), divide-and-conquer algorithms partition the problem into disjoint subproblems, solve the subproblems recursively and then combine their solutions to solve the original probblem.
In contrast, dynamic programming applies when the subproblems overlap - that is, when subproblems share subsubproblems.
In this context, a divide-and-conquer algorithms does more work than necessary, repeatedly solving the common subsubproblems.
A dynamic programming algorithm solves each subsubproblem just once, and saves itss answer in a table, thereby avoiding the work of recomputing the answer every time it solves each subsubproblem.

We typically apply dynamic programming to **optimization problems**.
Such problems can have many possible solutions.
Each solution has a value, and we wish to find a solution with the optimal (minimum or maximum) value.
We call such a solution _an_ optimal solution rather than _the_ solution, since there might be multiple such solutions that achieve the optimal value.

When developing a dynamic programming algorithm, we follow a sequence of four steps:
1. Characterize the structure of an optimal solution.
2. Recursively define the value of an optimal solution.
3. Compute the value of an optimal solution, typically in a bottom-up fashion.
4. Construct an optimal solution from computed information.

Steps 1-3 form the basis of a dynamic programming solution to a problem.
If we need only the value, and not the solution itself, we can omit step 4.
When we do perform step 4, we sometimes maintain additional information in step 3 to easily construct an optimal solution.

### Rod cutting

Our first example uses dynamic programming to solve a simple problem in deciding where to cut steel rods. 
Serling Enterprises buys long steel rods and cuts them into shorter rods, which it then sells. 
Each cut is free. 
The management of Serling Enterprises wants to know the best way to cut up the rods.

We assume that we know, for _i_ = 1, 2, ..., the price _p<sub>i</sub>_ in dollars that Sterling Enterprises charges for a rod of length _i_ inches.
Rod lengths are always an integral number of inches.

The **rod-cutting problem** is the following. 
Given a rod of length _n_ inches and a table of prices _p<sub>i</sub>_ for _i_ = 1, 2, ..., determine the maximum revenue _r<sub>n</sub>_ obtainable by cutting up the rod and selling the pieces.
Note that if price _p<sub>n</sub>_ for a rod of length _n_ is large enough, an optimal solution may require no cutting at all.

To solve the original problem of size _n_, we solve smaller problems of the same type, but of smaller sizes.
Once we make the first cut, we may consider the two pieces as independent instances of the rod-cutting problem.
The overall optimal solution incorporates optimal solutions to the two related subproblems, maximizing revenue from each of those two pieces.
We say that the rod-cutting problem exhibits **_optminal substructure_**: optimal solutions to a problem incorporate optimal solutions to related subproblems, which we may solve independently.

In a related, but slightly simpler, way to arrange a recursive structure for the rod-cutting problem, we view a decomposition as consisting of a first piece of length _i_ cut off the left-hand end, and then a right-hand remainder of length _n_ - _i_.
Only the remainder, and not the first piece, may be further divided.
We may view every decomposition of a length-_n_ rod in this way: as a first piece followed by some decomposition of the remainder.
When doing so, we can couch the solution with no cuts at all as saying that the first piece has size _i_ = _n_ andd revenue _p<sub>n</sub>_ and that the remainder has size 0 with corresponding revenue _r<sub>0</sub>_ = 0.
We thus obtain the following simpler version of equation:

_r<sub>n</sub>_ = max(_p<sub>i</sub>_ + _r<sub>n-1</sub>_).

In this formulation, an optimal solution embodies the solution to only _one_ related subproblem - the remainder - rather than two.

#### Recursive top-down implementation

The following procedure implements the computation implicit in the equation above in a straightforward, top-down, recursive manner.

>Cut-Rod(_p_, _n_)  
&nbsp;&nbsp;&nbsp;&nbsp;    **if** _n_ == 0  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        **return** 0  
&nbsp;&nbsp;&nbsp;&nbsp;    _q_ = -∞  
&nbsp;&nbsp;&nbsp;&nbsp;    **for** _i_ = 1 **to** _n_  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        _q_ = max(_q_, _p_[_i_] + Cut-Rod(_p_, _n_ - _i_))  
&nbsp;&nbsp;&nbsp;&nbsp;    **return** _q_

Procedure Cut-Rod takes as input an array _p_[1.._n_] of prices and an integer _n_, and it returns the maximum revenue possible for a rod of length _n_.

If you were to code up Cut-Rod in your favorite programming language and run it on your computer, you would find that once the input size becomes moderately large, your program would take a long time to run.
For _n_ = 40 you would find that your program takes at least several minutes, and most likely, more than an hour.
In fact, you would find that every time you increase _n_ by 1, your running time would approximately double.

The problem is that Cut-Rod calls itself recursively over and over again with the same parameter values, it solves the same subproblems repeatedly.

#### Using dynamic programming for optimal rod cutting

The dynamic-programming method works as follows.
Having observed that a naive recursive solution is inefficient because it solves the same subproblems repeatedly, we arrange for each subproblem to be solved only _once_, saving its solution.
If we need to refer to this subproblem's ssolution in the future, we can just look it up, rather than recompute it.
Dynamic programming thus uses additional memory to save computation time; it serves as an example of a **_time-memory trade-off_**.
The savings may be dramatic: an exponential time solution may be transformed into a polynomial time solution.
A dynamic-programming solution runs in polynomial time when the number of _distinct_ subproblems involved is polynomial in the input size and we can solve each subproblem in polynomial time.

There are usually two ways to implement a dynamic-programming approach.
We shall illustrate both of them with the rod-cutting example.

The first approach is **_top-down with memoization_**.
In this approach we write the procedure recursively in a natural manner, but modified to save the result of each subproblem (usually in an array or hashtable).
The procedure now first checks to see if it has previously solved this subproblem.
If so it returns the saved value, saving further computation at this level; if not the procedure computes the value in the usual manner.
We say that the recursive procedure has been **_memoized_**; it "remembers" what result it has computed previously.

The second approach is the **_bottom-up method_**.
This approach typically depends on some natural notion of the "size" of a subproblem, such that solving any particular subproblem depends only on solving "smaller" subproblems.
We sort the subproblems by size, and solve them in size order, smallest first.
When solving a particular subproblem, we have already solved all of the smaller subproblems its solution depends upon, and we have saved their solutions.
We solve every subproblem only once, and when we first see it, we have already solved all of its prerequisite subproblems.

These two approaches yield algorithms with the same asymptotic running time, except in unusual circumstances where the top-down approach does not actually recurse to examine all possible subproblems.
The bottom-up approach often has much better constant factors, since it has less overhead for procedure calls.

Here is the the pseudocode for the top-down Cut-Rod procedure, with memoization added:

>Memoized-Cut-Rod(_p_, _n_)  
&nbsp;&nbsp;&nbsp;&nbsp;    let _r_[0.._n_] be a new array  
&nbsp;&nbsp;&nbsp;&nbsp;    **for** _i_ = 0 **to** _n_  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        _r_[_i_] = -∞  
&nbsp;&nbsp;&nbsp;&nbsp;    **return** Memoized-Cut-Rod-Aux(_p_, _n_, _r_)  

>Memoized-Cut-Rod-Aux(_p_, _n_, _r_)  
&nbsp;&nbsp;&nbsp;&nbsp;    **if** _r_[_n_] ≥ 0  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        **return** _r_[_n_]  
&nbsp;&nbsp;&nbsp;&nbsp;    **if** _n_ == 0  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        _q_ = 0  
&nbsp;&nbsp;&nbsp;&nbsp;    **else** _q_ = -∞  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        **for** _i_ = 1 **to** _n_  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            _q_ = max(_q_, _p_[_i_] + Memoized-Cut-Rod-Aux(_p_, _n_ - 1, _r_))  
&nbsp;&nbsp;&nbsp;&nbsp;    _r_[_n_] = _q_  
&nbsp;&nbsp;&nbsp;&nbsp;    **return** _q_

**In Kotlin**: [Implementation](../src/main/kotlin/chapter15/MemoizedCutRod.kt) and [Tests](../src/test/kotlin/chapter15/MemoizedCutRodTest.kt).

Here the main procedure Memoized-Cut-Rod initializes a new auxiliary array _r_[0.._n_] with the value -∞, a convenient choice with which to denote "unknown." (Known revenue values are always nonnegative.)
It then calls its helper routing Memoized-Cut-Rod-Aux.

The procedure Memoized-Cut-Rod-Aux is just the memoized version of our previous procedure, Cut-Rod.
It first checks to see if the desired value is already known and, if it is, returns it.
Otherwise compute the desired value _q_ in the usual manner and saves it in _r_[_n_] and return it.

The bottom up version is even simpler:

>Bottom-Up-Cut-Rod(_p_, _n_)  
&nbsp;&nbsp;&nbsp;&nbsp;    let _r_[0.._n_] be a new array  
&nbsp;&nbsp;&nbsp;&nbsp;    _r_[0] = 0  
&nbsp;&nbsp;&nbsp;&nbsp;    **for** _j_ = 1 **to** _n_  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        _q_ = -∞  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        **for** _i_ = 1 **to** _j_  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            _q_ = max(_q_, _p_[_i_] + _r_[_j_ - _i_])  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        _r_[_j_] = _q_  
&nbsp;&nbsp;&nbsp;&nbsp;    **return** _r_[_n_]

**In Kotlin**: [Implementation](../src/main/kotlin/chapter15/BottomUpCutRod.kt) and [Tests](../src/test/kotlin/chapter15/BottomUpCutRodTest.kt).

For the bottom-up approach, Bottom-Up-Cut-Rod uses the natural ordering of the subproblems: a problem of size _i_ is "smaller" than a subproblem of size _j_ if _i_ < _j_.
Thus the procedure solves subproblems of size _j_ = 0, 1, ..., _n_, in that order.

We start by creating a new array _r_[0.._n_] in which to save the results of the subproblems, and initialize _r_[0] to 0, since a rod of length 0 earns no revenue.
Then we solve each subproblem of size _j_, for _j_ = 1, 2, ..., _n_, in order of increasing size.
The approach used to solve a problem of particular size _j_ is the same used by Cut-Rod, except that we directly reference array entry _r_[_j_ - _i_] instead of making a recursive call to solve the subproblem of size _j_ - _i_.
We then save the solution of size _j_ in _r_[_j_].
And finally, return _r_[_n_], which equals optimal value _r<sub>n</sub>_.

The bottom-up and top-down versions have the same asymptotic running time.
The running time of procedure Bottom-Up-Cut-Rod is _Θ_(_n_<sup>2</sup>), due to its doubly-nested loop structure.
The number of iterations of its inner **for** loop forms an arithmetic series.
The running time of its top-down counterpart, Memoized-Cut-Rod, is also _Θ_(_n_<sup>2</sup>), although this running time may be a little harder to see.
Because a recursive call to solve a previously solved subproblem returns immediately, Memoized-Cut-Rod solves each subproblem just once.
To solve a subproblem of size _n_, the **for** loop iterates _n_ times.

#### Subproblem graphs

When thinking about a dynamic-programming problem, we should understand the set of subproblems involved and how subproblems depend on one another.

#### Reconstructing a solution

Our dynamic-programming solutions to the rod-cutting problem return the value of an optiomal solution, but they do not return an actual solution: a list of piece sizes.
We can extend the dynamic programming approach to record not only the optimal _value_ computed for each subproblem, but also a _choice_ that led to the optimal value. 
With this information we can print out an optimal solution.

Below an extended version of the Bottom-Up-Cut-Rod that computes, for each rod size _j_, not only the maximum revenue _r_<sub>_j_</sub>, but also _s_<sub>_j_</sub>, the optimal size of the ffirst piece to cut off:

>Extended-Bottom-Up-Cut-Rod(_p_, _n_)  
&nbsp;&nbsp;&nbsp;&nbsp;    let _r_[0.._n_] and _s_[0.._n_] be new arrays  
&nbsp;&nbsp;&nbsp;&nbsp;    _r_[0] = 0  
&nbsp;&nbsp;&nbsp;&nbsp;    **for** _j_ = 1 **to** _n_  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        _q_ = -∞  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        **for** _i_ = 1 **to** _j_  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            **if** _q_ < _p_[_i_] + _r_[_j_ - _i_]  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            _q_ = _p_[_i_] + _r_[_j_ - _i_]  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            _s_[_j_] = _i_  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        _r_[_j_] = _q_  
&nbsp;&nbsp;&nbsp;&nbsp;    **return** _r_ and _s_

**In Kotlin**: [Implementation](../src/main/kotlin/chapter15/ExtendedBottomUpCutRod.kt) and [Tests](../src/test/kotlin/chapter15/ExtendedBottomUpCutRodTest.kt).

This procedure is similar to Bottom-Up-Cut-Rod, except that it creates the array _s_, and it updates _s_<sub>_j_</sub> to hold the optimal size _i_ of the first piece to cut off when solving a subproblem of size _j_.

The following procedure takes a price table _p_ and a rod size _n_, and it calls Extended-Bottom-Up-Cut-Rod to compute the array _s_[1.._n_] of optimal first-piece sizes and then prints out the complete list of piece sizes in an optimal decomposition of rod of length _n_:

>Print-Cut-Rod-Solution(_p_, _n_)  
&nbsp;&nbsp;&nbsp;&nbsp;    (_r_, _s_) = Extended-Bottom-Up-Cut-Rod(_p_, _n_)  
&nbsp;&nbsp;&nbsp;&nbsp;    **while** _n_ > 0  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        print _s_[_n_]  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        _n_ = _n_ - _s_[_n_]  

**In Kotlin**: [Implementation](../src/main/kotlin/chapter15/PrintCutRodSolution.kt) and [Tests](../src/test/kotlin/chapter15/PrintCutRodSolutionTest.kt).

In our rod-cutting example, the call Extended-Bottom-Up-Cut-Rod(_p_, 10) would return the following arrays:

|   _i_    | 0 | 1 | 2 | 3 | 4  | 5  | 6  | 7  | 8  | 9  | 10 |
|:--------:|---|---|---|---|----|----|----|----|----|----|----|
| _r_[_i_] | 0 | 1 | 5 | 8 | 10 | 13 | 17 | 18 | 22 | 25 | 30 |
| _s_[_i_] | 0 | 1 | 2 | 3 | 2  | 2  | 6  | 1  | 2  | 3  | 10 |

A call to Print-Cut-Rod-Solution(_p_, 10) would print just 10, but a call with _n_ = 7 would print the cuts 1 and 6, corresponding to the first optimal decomposition for _r_<sub>7</sub> given earlier.
